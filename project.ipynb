{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Traffic Mortality in the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code cell without any tag. You can put convenience code here,\n",
    "# but it won't be included in any way in the final project.\n",
    "# For example, to be able to run tests locally in the notebook\n",
    "# you need to install the following:\n",
    "# pip install nose\n",
    "# pip install git+https://github.com/datacamp/ipython_nose\n",
    "# and then load in the ipython_nose extension like this:\n",
    "%load_ext ipython_nose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 1. Identifying the raw data files and determining their format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "![](img/car-accident.jpg)\n",
    "\n",
    "While the rate of fatal road accidents rate have been decreasing steadily since the 80's, the past 10 years have seen a stagnation in this reduction. Coupled with the increase number of miles driven in the nation, the total number of traffic related fatalities has now reached a 10 year high and is rapidly increasing.\n",
    "\n",
    "\n",
    "An exciting intro to the analysis. Provide context on the problem you're going to solve, the dataset(s) you're going to use, the relevant industry, etc. You may wish to briefly introduce the techniques you're going to use. Tell a story. Get students excited! This will show up in the student's notebook. It should at most have 1200 characters and/or 4 paragraphs.\n",
    "\n",
    "The most common error instructors make in **context cells** is referring to the student or the Project. We want Projects notebooks to appear as a blog post or a data analysis. Bad: *\"In this Project, you will...\"* Good: *\"We will...\"* Save the instruction for the `@instructions`, `@hint`, and `@tests` cells, where you can refer the student and the Project.\n",
    "\n",
    "Images are welcome additions to every `@context` cell, but especially this first one. Make sure the images you use have a [permissive license]() then store them in the `img` folder in your GitHub repository and display them using [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#images).\n",
    "\n",
    "---\n",
    "\n",
    "While the rate of fatal road accidents rate have been decreasing steadily since the 80's, the past 10 years have seen a stagnation in this reduction. Coupled with the increase number of miles driven in the nation, the total number of traﬃc related fatalities has now reached a 10 year high and is rapidly increasing.\n",
    "\n",
    "![](img/accident-history.png)\n",
    "\n",
    "Per request of the US Department of Transportation we are currently investigating how to derive a strategy to reduce the incidence of road accidents across the nation. By looking at the demographics of traﬃc accident victims for each US state, we find that there is a lot of variation between states. Now we want to understand if there are patterns in this variation in order to derive suggestions for a policy action plan. In particular, instead of implementing a costly nation-wide plan we want to focus on groups of  states with similar profiles. How can we find such groups in a statistically sound way and communicate the result effectively?\n",
    "\n",
    "To accomplish these tasks, we will make use of data wrangling, plotting, dimensionality reduction, and unsupervised clustering.\n",
    "\n",
    "The data given to us was originally collected by the National Highway Traﬃc Safety Administration and the National Association of Insurance Commissioners. This particular dataset was compiled and released as a [CSV-ﬁle](https://github.com/ﬁvethirtyeight/data/tree/master/bad-drivers) by FiveThirytEight under the [CC-BY4.0 license](https://github.com/ﬁvethirtyeight/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Explore your current folder and view the main dataset file.\n",
    "\n",
    "- Check the name of the current folder using `!pwd`\n",
    "- List all files in this folder using `!ls`\n",
    "- View the first 20 lines of `road-accidens.csv` in the `datasets` folder using `!head`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Good to know\n",
    "\n",
    "This Project lets you practice the skills from [Introduction to shell for data science](https://www.datacamp.com/courses/introduction-to-shell-for-data-science) including how to navigate the file system and view files; [pandas Foundations](https://www.datacamp.com/courses/pandas-foundations) including reading, exploring, filtering, and grouping data; [Manipulating data frames with pandas](https://www.datacamp.com/courses/manipulating-dataframes-with-pandas) including how to reshape data into the long format and how to perform multiple aggregations; [Merging data frames with pandas](https://www.datacamp.com/courses/merging-dataframes-with-pandas) including how two merge two dataframes; [Unsupervised Learning in Python](https://www.datacamp.com/courses/unsupervised-learning-in-python) including KMeans clustering, dimensionally reduction through PCA, and visualizations using `matplotlib`; [Supervised Learning with scikit-learn](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn) including multivariate regression; [Intermediate Python for Data Science](https://www.datacamp.com/courses/intermediate-python-for-data-science) including visualizations using `matplotlib`; and [Data Visuazliation in seaborn](https://www.datacamp.com/courses/data-visualization-with-seaborn) inclusing statistical visualizations using `seaborn`. We recommend that you review the appropriate sections of those courses before starting this Project.\n",
    "\n",
    "\n",
    "Helpful links:\n",
    "- [Manipulating Files and Directories](https://www.datacamp.com/courses/introduction-to-shell-for-data-science).\n",
    "- [How to run shell commands in a Jupyter Notebook](https://jakevdp.github.io/PythonDataScienceHandbook/01.05-ipython-and-shell-commands.html) (through the underlying IPython insterpreter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Preface the shell command with `!` so that the Jupyter Notebook knows to interpret it as a shell command rather than a Python variable, e.g. `!ls` to list files in the directory.\n",
    "\n",
    "For the last part of this task, study excersice [number 3](https://campus.datacamp.com/courses/introduction-to-shell-for-data-science/manipulating-data?ex=3) and [number 5](https://campus.datacamp.com/courses/introduction-to-shell-for-data-science/manipulating-data?ex=5) in the \n",
    "[Manipulating Files and Directories](https://www.datacamp.com/courses/introduction-to-shell-for-data-science) lecture to find out how to view the first few lines of a file using shell commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the name of the current folder\n",
    "current_dir = ...\n",
    "print(current_dir)\n",
    "\n",
    "# List all files in this folder\n",
    "file_list = ...\n",
    "print(file_list)\n",
    "\n",
    "# List all files in the datasets directory\n",
    "dataset_list = ...\n",
    "print(dataset_list)\n",
    "\n",
    "# View the first 20 lines of `road-accidens.csv` in the `datasets` folder\n",
    "accidents_head = ...\n",
    "accidents_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the name of the current directory\n",
    "current_dir = !pwd\n",
    "print(current_dir)\n",
    "\n",
    "# List all files in this directory\n",
    "file_list = !ls\n",
    "print(file_list)\n",
    "\n",
    "# List all files in the datasets directory\n",
    "dataset_list = !ls datasets\n",
    "print(dataset_list)\n",
    "\n",
    "# View the first 20 lines of `road-accidens.csv` in the `datasets` folder\n",
    "accidents_head = !head -n 20 datasets/road-accidents.csv\n",
    "accidents_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def test_current_dir():\n",
    "    assert current_dir == [str(Path.cwd())], \\\n",
    "    'The current_dir variable was not correctly assigned'\n",
    "    \n",
    "    \n",
    "def test_file_list():\n",
    "    assert sorted(file_list) == sorted([str(p) for p in list(Path('.').glob('[A-z]*'))]), \\\n",
    "    'The file_list variable was not correctly assigned'\n",
    "    \n",
    "    \n",
    "def test_accidents_head():\n",
    "    with open('datasets/road-accidents.csv') as f:\n",
    "        accidents_head_test = []\n",
    "        for i in range(20):\n",
    "            accidents_head_test.append(f.readline().rstrip())\n",
    "    assert accidents_head == accidents_head_test, \\\n",
    "    'The accidents_head variable was not correctly assigned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 2. Reading in and getting an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "After peeking at the beginning of the file, we now know the data format and the next step is to import the data as a dataframe. After this, we will orient ourselves to get to know the data we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Read in the main dataset file and start exploring the data.\n",
    "\n",
    "- Import the `pandas` module as \"pd\".\n",
    "- Read in `road-accidents.csv` using `read_csv()` from `pandas`. Read the documentation on how to use the `comment` and `sep` parameters.\n",
    "- Save the number of rows columns as a tuple, using the `shape` attribute.\n",
    "- Generate an overview of the data frame using the `info()` method. This overview should include the number of rows and columns, the column data names and data types\n",
    "- Display the last five rows of the data frame using the `head()` method. Does the output make sense? A quick data type sanity check like this can save us major headaches down the line.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- pandas [cheat sheet](http://datacamp-community.s3.amazonaws.com/fbc502d0-46b2-4e1b-b6b0-5402ff273251)\n",
    "- pandas `read_csv()` function [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)\n",
    "- Reading a flat file [exercise](https://campus.datacamp.com/courses/pandas-foundations/data-ingestion-inspection?ex=10) in the pandas Foundations course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Remember that the dataset is located within the `datasets` folder so the full path from the current directory to the dataset is `'datasets/road-accidents.csv'`. From the output of the `head` command in the previus task, we can see that there are comments in the csv-file that are prefixed with `#` and the separators for the values is `|` rather than `,`. Look into the documentation of `read_csv` (with `pd.read_csv?`), to find out which parameters to use to specify `'#'` for comments and `'|'` as the separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the `pandas` module as \"pd\"\n",
    "# ... YOUR CODE FOR TASK 2 ...\n",
    "\n",
    "# Read in `road-accidents.csv`\n",
    "car_acc = ...\n",
    "\n",
    "# Save the number of rows columns as a tuple\n",
    "rows_and_cols = ...\n",
    "print('There are {} rows and {} columns.\\n'.format(\n",
    "    rows_and_cols[0], rows_and_cols[1]))\n",
    "\n",
    "# Generate an overview of the data frame\n",
    "car_acc_information = ...\n",
    "print(car_acc_information)\n",
    "\n",
    "# Display the last five rows of the data frame.\n",
    "# ... YOUR CODE FOR TASK 2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the `pandas` module as \"pd\"\n",
    "import pandas as pd\n",
    "\n",
    "# Read in `road-accidents.csv`\n",
    "car_acc = pd.read_csv('datasets/road-accidents.csv', comment='#', sep='|')\n",
    "\n",
    "# Save the number of rows columns as a tuple\n",
    "rows_and_cols = car_acc.shape\n",
    "print('There are {} rows and {} columns.\\n'.format(\n",
    "    rows_and_cols[0], rows_and_cols[1]))\n",
    "\n",
    "# Generate an overview of the data frame\n",
    "car_acc_information = car_acc.info()\n",
    "print(car_acc_information)\n",
    "\n",
    "# Display the last five rows of the data frame.\n",
    "car_acc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def test_pandas_import():\n",
    "    assert 'pandas' in list(sys.modules.keys()), \\\n",
    "        'The pandas module has not been imported correctly'\n",
    "    \n",
    "\n",
    "def test_car_acc():\n",
    "    car_acc_test = pd.read_csv('datasets/road-accidents.csv', comment='#', sep='|')\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(car_acc, car_acc_test)\n",
    "    except AssertionError:\n",
    "        assert False, \"The car_acc data set was not read in correctly.\"\n",
    "        \n",
    "        \n",
    "def test_car_acc_shape():\n",
    "    assert rows_and_cols == (51, 5), \\\n",
    "    'The number of rows and variables were not calculated correctly'\n",
    "    \n",
    "\n",
    "def car_acc_info():\n",
    "    car_acc_information = car_acc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 3. Creating a textual and a graphical summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "We now have an idea of what the dataset looks like. To further familiarize ourselves with this data, we will calculate summary statistics and produce a graphical overview of the data. The graphical overview is good to get a sense for the distribution of variables within the data, and could consist of one histogram per column. It is often a good idea to also explore the pairwise relationsship between all columns in the data set by using a using pairwise scatterplots (sometimes referred to as a \"scatterplot matrix\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Create a textual and graphical overview of the data.\n",
    "\n",
    "- Compute the summary statistics of all columns in the `car_acc` dataframe, using the `describe()` method.\n",
    "- Create a pairwise scatterplot to explore the data, using `sns.pairplot()`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- [sns.pairplot lecture](https://campus.datacamp.com/courses/data-visualization-with-seaborn/creating-plots-on-data-aware-grids?ex=5)\n",
    "- [seaborn pairplot documentation](https://seaborn.pydata.org/generated/seaborn.pairplot.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "`sns.pairplot` takes one argument: the variable name of data frame to be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# seaborn is imported as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# This line is needed for the plots to appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Compute the summary statistics of all columns in the `car_acc` dataframe\n",
    "sum_stat_car = ...\n",
    "\n",
    "# Create a pairwise scatterplot to explore the data\n",
    "# ... YOUR CODE FOR TASK 3 ...\n",
    "\n",
    "sum_stat_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# seaborn is imported as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# This line is needed for the plots to appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Compute the summary statistics of all columns in the `car_acc` dataframe\n",
    "sum_stat_car = car_acc.describe()\n",
    "\n",
    "# Create a pairwise scatterplot to explore the data\n",
    "sns.pairplot(car_acc)\n",
    "\n",
    "sum_stat_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def test_seaborn_import():\n",
    "    assert 'seaborn' in list(sys.modules.keys()), \\\n",
    "        'The seaborn module has not been imported correctly'\n",
    "    \n",
    "\n",
    "def test_car_desc():\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(sum_stat_car, car_acc.describe())\n",
    "    except AssertionError:\n",
    "        assert False, \"The sum_stat_car variable was not created correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 4. Quantifying the association of features and fatal accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "We can already see some potentially intersting relationships between the target variable (the number of fatal accidents) and the feature variables (the remaining three columns).\n",
    "\n",
    "To quantify the pairwise relationships that we observed in the scatterplots, we can compute the Pearson correlation coefficient matrix. The Pearson correlation coeffcient is one of the most common methods to quantify correlation between variables and by convention the following thresholds are usually used:\n",
    "\n",
    "- 0.2 = weak\n",
    "- 0.5 = medium\n",
    "- 0.8 = strong\n",
    "- 0.9 = very strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Explore the correlation between all column pairs in the data frame.\n",
    "\n",
    "- Compute the correlation coefficent for all column pairs, using the `corr()` method. By deafult, the pearson correlation coefficient will be computed.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "\n",
    "- [pandas corr method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Call the `corr()` method on the dataframe without any arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the correlation coefficent for all column pairs\n",
    "corr_columns = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the correlation coefficent for all column pairs\n",
    "corr_columns = car_acc.corr()\n",
    "corr_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "def test_corr_columns():\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(corr_columns, car_acc.corr())\n",
    "    except AssertionError:\n",
    "        assert False, \"The corr_columns variable was not created correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 5. Fitting a multivariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "From the correlation table we see that the amount of fatal accidents is most strongly correlated with alcohol consumption (first row). But in addition, we also see that some of features are correlated with each other, for instance speeding and alcohol consumption are positively correlated. We therefore want to compute the association of the target with each feature while adjusting for the effect of the remaining features. This can be done using a multivariate linear regression.\n",
    "\n",
    "Both the multivariate regression and the correlation measure how strongly the features are associated with the outcome (fatal accidents). When comparing the regression coefficients with the correlation coefficients we will see that they are slightly different. The reason for this is that the multiple regression computes the association of a feature with an outcome, given the association with all other features, which is not accounted for when calculating the correlation coefficients.\n",
    "\n",
    "A particularly interesting case is when the correlation coefficient and theregression coefficient of the same feature have opposite signs. How can this be?For example, when a feature A is positively correlated with the outcome Y butalso positively correlated with a different feature B that has a negative effecton Y, then the indirect correlation (A->B->Y) can overwhelm the directcorrelation (A->Y). In such a case, the regression coefficient of featureA could be positive, while the correlation coefficient is negative. This is sometimes called a *masking* relationship. Let’s see if the multivariate regression can reveal such a phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Fit a multivariate linear regression model using the fatal accident rate as the outcome. \n",
    "\n",
    "- Import the `linear_model` function from `sklearn`.\n",
    "- Create the features and target data frames, by subsetting the data frame `car_acc`.\n",
    "- Create a linear regression object, using `linear_model.LinearRegression()`.\n",
    "- Fit a multivariate linear regression model, using `fit()`.\n",
    "- Retrieve the regression coefficients from the `coef_` attritube of the fitted regression object.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "\n",
    "- [scikit-learn linear regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "The `features` dataframe should contain the following columns `'perc_fatl_speed'`, `'perc_fatl_alcohol'`, `'perc_fetl_1st_time'`. These columns need to be passed as a list to subset the orignal dataframe.\n",
    "\n",
    "The `'target'` dataframe should contain the `'drvr_fatl_col_bmiles'` column.\n",
    "\n",
    "To see the help documentation for how to fit the regression, use `fit.reg?`. Remember that the `X` parameter corresponds to the features and the `y` parameter is the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the linear model function from sklearn\n",
    "from sklearn import ...\n",
    "\n",
    "# Create the features and target data frames\n",
    "features = ...\n",
    "target = ...\n",
    "\n",
    "# Create a linear regression object\n",
    "reg = ...\n",
    "\n",
    "# Fit a multivariate linear regression model\n",
    "# ... YOUR CODE FOR TASK 5 ...\n",
    "\n",
    "# Retrieve the regression coefficients\n",
    "fit_coef = ...\n",
    "fit_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the linear model function from sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Create the features and target data frames\n",
    "features = car_acc[['perc_fatl_speed', 'perc_fatl_alcohol', 'perc_fatl_1st_time']]\n",
    "target = car_acc['drvr_fatl_col_bmiles']\n",
    "\n",
    "# Create a linear regression object\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# Fit a multivariate linear regression model\n",
    "reg.fit(features, target)\n",
    "\n",
    "# Retrieve the regression coefficients\n",
    "fit_coef = reg.coef_\n",
    "fit_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "\n",
    "def test_sklearn_import():\n",
    "    assert 'sklearn' in list(sys.modules.keys()), \\\n",
    "        'The seaborn module has not been imported correctly'\n",
    "    \n",
    "    \n",
    "def test_features_df():\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(features, car_acc[['perc_fatl_speed', 'perc_fatl_alcohol', 'perc_fatl_1st_time']])\n",
    "    except AssertionError:\n",
    "        assert False, \"The features data frame was not created correctly.\"\n",
    "\n",
    "        \n",
    "def test_target_df():\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(target.to_frame(), car_acc[['drvr_fatl_col_bmiles']])\n",
    "    except AssertionError:\n",
    "        assert False, \"The target data frame variable was not created correctly.\"\n",
    "        \n",
    "        \n",
    "def test_lin_reg():\n",
    "    assert reg.coef_.round(3).tolist() == [-0.042,  0.191,  0.025], \\\n",
    "     'The linear regression coeffiecients are not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 6. Performing PCA on standardized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "We have learned that alcohol consumption is weakly associated with the amount of fatal accidents across states. This could lead you to conclude that alcohol consumption should be a focus for futher investigations and maybe strategies should divide states into high versus low alcohol consumption in accidents. But there are also associations between  alcohol consumptions and the other two features, so it might be worth trying to split the states in a way that accounts for all three features.\n",
    "\n",
    "One way of clustering the data is to use PCA to visualize data in reduced dimensional space where we can try to pickup patterns by eye. PCA uses the absolute variance to calculate the overall variance explained for each principal component, so it is important that the features are on a similar scale (unless we would have a particular reason that one features should be weighted more).\n",
    "\n",
    "We'll use the appropriate scaling function to standardize the features to be centered with mean 0 and scaled with standard deviation 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Perform a principal component analysis on the standardized data\n",
    "\n",
    "- Standardize and center the feature columns, using `StandardScaler` from `sklearn`.\n",
    "- Import the PCA function from `sklearn`.\n",
    "- Fit the standardized data to the pca using `fit`.\n",
    "- Plot the proportion of variance explained as bar plot using `matplotlib.pyplot.bar`.\n",
    "- Compute the cumulative proportion of variance explained by the first two principal components from `pca.explained_variance_ratio_`. Here the two first componenets can simply be added together or the cumulative summation method (`cumsum`) from `pandas` can be used.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "\n",
    "- [scikit-learn standard scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "- [scikit-learn PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- [Visualizing the PCA variance explained](https://campus.datacamp.com/courses/unsupervised-learning-in-python/decorrelating-your-data-and-dimension-reduction?ex=5)\n",
    "- [PCA variance explained excersice](https://campus.datacamp.com/courses/unsupervised-learning-in-python/decorrelating-your-data-and-dimension-reduction?ex=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a DataFrame named `my_data` like this after importing the pandas library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "my_data = pd.read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Standardize and center the feature columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled = ...\n",
    "\n",
    "# Import the PCA function from sklearn\n",
    "# ... YOUR CODE FOR TASK 6 ...\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the standardized data to the pca\n",
    "# ... YOUR CODE FOR TASK 6 ...\n",
    "\n",
    "# Plot the proportion of variance explained as bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(pca.n_components_),  ...)\n",
    "\n",
    "# Compute the cumulative proportion of variance explained by the first two principal components\n",
    "two_first_comp_var_exp = ...\n",
    "print(\"The cumulative variance of the first two principal componenets is {}\".format(\n",
    "    round(two_first_comp_var_exp, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Standardize and center the feature columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Import the PCA function from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the standardized data to the pca\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Plot the proportion of variance explained as bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(pca.n_components_),  pca.explained_variance_ratio_)\n",
    "\n",
    "# Compute the cumulative proportion of variance explained by the first two principal components\n",
    "two_first_comp_var_exp = pca.explained_variance_ratio_.cumsum()[1]\n",
    "print(\"The cumulative variance of the first two principal componenets is {}\".format(\n",
    "    round(two_first_comp_var_exp, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "\n",
    "def test_scaler():\n",
    "    assert scaler.fit_transform(features).round(3).tolist()[-1] == [1.077, 0.259, 0.185], \\\n",
    "        'The scaled features were not calculated properly'\n",
    "\n",
    "    \n",
    "def test_pca():\n",
    "    assert (pca.explained_variance_ratio_ == PCA().fit(features_scaled).explained_variance_ratio_).all(), \\\n",
    "        'The explained variance ratio for the PCA was not correctly calculated'\n",
    "    \n",
    "    \n",
    "def test_pc1_pc2():\n",
    "    assert two_first_comp_var_exp == PCA().fit(features_scaled).explained_variance_ratio_.cumsum()[1], \\\n",
    "        'The cumulative sum for the explained variance of the two first principal components was not correctly calculated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 7. Visualizing the data using the two first principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "The first two principal components enable visualization of the data in two dimensions while capturing a high proportion of the variation (79%) from all three features: speeding, alcohol influence, and first time accidents. This enables us to use our eyes to try to discern patterns in the data with the goal to find groups of similar states. Although clustering algorithms are becoming increasingly efficient, human pattern recognition is an easy accessible and very efficient method of assessing patterns in data.\n",
    "\n",
    "We will create a scatter plot of the first principle components and explore how the states cluster together in this visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Transform the data and visualize the first two principal components in a scatter plot.\n",
    "- Transform the data using two principal components, and the `fit_transform()` method of the `pca` object.\n",
    "- Plot the first two principal components in a scatter plot, using `matplotlib.pyplot.scatter`.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a DataFrame named `my_data` like this after importing the pandas library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "my_data = pd.read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the data using two principal components\n",
    "pca = ...\n",
    "p_comps = ...\n",
    "\n",
    "# Plot the first two principal components in a scatter plot\n",
    "# ... YOUR CODE FOR TASK 7 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the data using two principal components\n",
    "pca = PCA(n_components=2)\n",
    "p_comps = pca.fit_transform(features_scaled)\n",
    "# Plot the first two principal components in a scatter plot\n",
    "plt.scatter(p_comps[:, 0], p_comps[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "def test_pca_trans():\n",
    "    assert (p_comps == PCA(n_components=2).fit_transform(features_scaled)).all(), \\\n",
    "    'The student will see this message if the test fails'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 8. Finding clusters of similar states in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "It was not entirely clear from the PCA scatter plot how many groups the states cluster in. To assist with identifying a reasonable number of clusters, we can use KMeans clustering by creating a scree plot and finding the \"elbow\", which is an indication of when the addition of more clusters does not add much explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Cluster the states using the KMeans algorithm and visualize the explanatory power for different numbers of clusters.\n",
    "\n",
    "- Import KMeans from sklearn\n",
    "- Initialize the KMeans object using the current number of clusters (k)\n",
    "- Fit the scaled features to the KMeans object\n",
    "- Append the inertia for `km` to the list of inertias\n",
    "- Plot the results in a line plot using `matplotlib.pyplot.plot`. This type of plot is also called a \"scree plot\".\n",
    "\n",
    "<hr>\n",
    "\n",
    "Provide more info (if necessary) and include links to external resources under the horizontal ruler. The instructions should at most have 600 characters. Example format for links below.\n",
    "\n",
    "Helpful links:\n",
    "- [scikit-learn KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "- [Evaluating a clustering](https://campus.datacamp.com/courses/unsupervised-learning-in-python/clustering-for-dataset-exploration?ex=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a DataFrame named `my_data` like this after importing the pandas library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "my_data = pd.read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn\n",
    "# ... YOUR CODE FOR TASK 8 ...\n",
    "\n",
    "# A loop will be used to plot the explantory power for up to 10 KMeans clusters\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Initialize the KMeans object using the current number of clusers (k)\n",
    "    km = KMeans(n_clusters=..., random_state=8)\n",
    "    # Fit the scaled features to the KMeans object\n",
    "    km.fit(...)\n",
    "    # Append the inertia for `km` to the list of inertias\n",
    "    inertias.append(...)\n",
    "    \n",
    "# Plot the results in a line plot\n",
    "plt.plot(..., ..., marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# A loop will be used to plot the explantory power for up to 10 KMeans clusters\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Initialize the KMeans object using the current number of clusers (k)\n",
    "    km = KMeans(n_clusters=k, random_state=8)\n",
    "    # Fit the scaled scaled features to the KMeans object\n",
    "    km.fit(features_scaled)\n",
    "    # Append the inertia for `km` to the list of inertias\n",
    "    inertias.append(km.inertia_)\n",
    "    \n",
    "# Plot the results in a line plot\n",
    "plt.plot(ks, inertias, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "def test_inertias():\n",
    "    test_ins = [153.0, 101.591, 72.293, 57.791, 46.106, 39.213, 32.99, 29.381, 25.985]\n",
    "    assert [round(inertia, 3) for inertia in inertias] ==  test_ins, \\\n",
    "        'The list of inertias was not properly constructed'\n",
    "\n",
    "    \n",
    "def test_km():\n",
    "    assert (km.labels_ == KMeans(n_clusters=k, random_state=8).fit(features_scaled).labels_).all(), \\\n",
    "        'The KMeans labels were not properly assigned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 9. Using KMeans to visualize clusters in the PCA scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "Since there wasn't a clear elbow in the scree plot, assigning the states to either 2 or 3 clusters is a reasonable choice and we will resume our analysis using 3 clusters. Let's see how the PCA scatter plot looks if we color the states according to the cluster they are assigned to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Highlight the clusters of the K-means fit with three clusters in the PCA scatterplot\n",
    "\n",
    "- Create a KMeans object with 3 clusters, use random state number 8 like in the previous task.\n",
    "- Fit the data to the `km` object.\n",
    "- Create a scatterplot of the first two principal components and color it according to the KMeans cluster assignment.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- [matplotlib scatter plot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a DataFrame named `my_data` like this after importing the pandas library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "my_data = pd.read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a KMeans object with 3 clusters, use random_state=8 \n",
    "km = ...\n",
    "\n",
    "# Fit the data to the `km` object\n",
    "#  ... YOUR CODE FOR TASK 9 ...\n",
    "\n",
    "# Create a scatterplot of the first two principal components\n",
    "# and color it according to the KMeans cluster assignment \n",
    "# ... YOUR CODE FOR TASK 9 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a KMeans object with 3 clusters \n",
    "km = KMeans(n_clusters=3, random_state=8)\n",
    "# Fit the data to the `km` object\n",
    "km.fit(features_scaled)\n",
    "# Create a scatterplot of the first two principal components\n",
    "# and color it according to the KMeans cluster assignment \n",
    "plt.scatter(p_comps[:, 0], p_comps[:, 1], c=km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "def test_km_labels():\n",
    "    assert (km.labels_ == KMeans(n_clusters=3, random_state=8).fit(features_scaled).labels_).all(), \\\n",
    "        'The KMeans labels were not properly assigned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 10. Visualizing the feature differences between the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "Thus far, we have used both our visual interpretation of the data and the KMeans clustering algorithm to reveal patterns in the data, but what do these patterns mean?\n",
    "\n",
    "Remember that the information we have used to cluster the states into three distinct groups are the percentage of drivers speeding, under alcohol influence and that has not previously been involved in an accident. We used these clusters to visualize how the states group together when considering the first two principal components. This is good for us to understand structure in the data, but not always easy to understand, especially not if the findings are to be communicated to a non-specialist audience.\n",
    "\n",
    "A reasonable next step in our analysis is to explore how the three clusters are different in terms of the three features that we used for clustering. Instead of using the scaled features, we return to using the unscaled features to help us interpret the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Visualise the distribution of speeding, alcohol influence and precentage of first time accidents in a direct comparison of the clusters\n",
    "\n",
    "- Create a new column with the labels from the KMeans clustering, using `km.labels_`.\n",
    "- Reshape the data frame to the long format, using `pd.melt()`. Use the features as the value variables and give them the name name 'measurement' in the new data framt. Name the value column 'percent'.\n",
    "- Create a violinplot splitting and coloring the results according to the km-clusters using the `hue` parameter. Plot the measurements along the y-axis and the percent values along the x-axis.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- [Creating long data frames in pandas](https://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/rearranging-and-reshaping-data?ex=9)\n",
    "- [seaborn violinplots](https://seaborn.pydata.org/generated/seaborn.violinplot.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a DataFrame named `my_data` like this after importing the pandas library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "my_data = pd.read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a new column with the labels from the KMeans clustering\n",
    "car_acc['cluster'] = ...\n",
    "\n",
    "# Reshape the data frame to the long format\n",
    "melt_car = pd.melt(...)\n",
    "\n",
    "# Create a violinplot splitting and coloring the results according to the km-clusters\n",
    "sns.violinplot(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a new column with the labels from the KMeans clustering\n",
    "car_acc['cluster'] = km.labels_\n",
    "\n",
    "# Reshape the data frame to the long format\n",
    "melt_car = pd.melt(car_acc, id_vars='cluster', var_name='measurement', value_name='percent',\n",
    "        value_vars=['perc_fatl_alcohol', 'perc_fatl_speed', 'perc_fatl_1st_time'])\n",
    "\n",
    "# Create a violinplot splitting and coloring the results according to the km-clusters\n",
    "sns.violinplot(y='measurement', x='percent', data=melt_car, hue='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "def test_melt():\n",
    "    test_melt = pd.melt(car_acc, id_vars='cluster', var_name='measurement', value_name='percent',\n",
    "       value_vars=['perc_fatl_alcohol', 'perc_fatl_speed', 'perc_fatl_1st_time'])\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(melt_car, test_melt)\n",
    "    except AssertionError:\n",
    "        assert False, \"The melt_car data frame was not created correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 11. Computing the number of accidents within each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "Now it is clear that different groups of states may require different interventions. Since resources and time are limited, it is useful to start off with an intervention in one of the three groups first. Which group would this be? To determine this, we will include data on how many miles are driven in each state, because this will help us to compute the total number of fatal accidents in each state. Data on miles driven is available in another tab-delimited text file. We will assign this new information to a column in the data frame and create a violin plot for how many total fatal traffic accidents there are within each state cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Add data on the number of miles driven per state to compute total number of fatal accidents and total accidents for each clusters.\n",
    "\n",
    "- Read in the `miles-drives.csv` using the same column separator as before.\n",
    "- Merge the `miles_driven` data frame with the `car_acc` data frame. Merge on the common column `state`.\n",
    "- Create a new column for the number of drivers involved in fatal accidents. Use the columns 'drvr_fatl_col_bmiles' and 'million_miles_annuall', note that these are in billions and million respectively.\n",
    "- Create a barplot of the total number of accidents per cluster, using the `estimator` parameter. Don't include a confidence interval.\n",
    "- Calculate the number of states in each cluster and their mean and sum, using the pandas data frame `agg()` method.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Helpful links:\n",
    "- [Excercise on merging pandas data frames on a column](https://campus.datacamp.com/courses/merging-dataframes-with-pandas/merging-data?ex=3)\n",
    "- [Performing multiple aggregations in pandas data frames](https://campus.datacamp.com/courses/manipulating-dataframes-with-pandas/grouping-data?ex=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a DataFrame named `my_data` like this after importing the pandas library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "my_data = pd.read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Read in the `miles-drives.csv`\n",
    "miles_driven = ...\n",
    "\n",
    "# Merge the `miles_driven` data frame with the `car_acc` data frame\n",
    "car_acc_miles = pd.merge(...)\n",
    "\n",
    "# Create a new column for the number of drivers involved in fatal accidents\n",
    "car_acc_miles['num_drvr_fatl_col'] = ...\n",
    "\n",
    "# Create a barplot of the total number of accidents per cluster\n",
    "sns.barplot(...)\n",
    "\n",
    "# Calculate the number of states in each cluster and their mean and sum.\n",
    "count_mean_sum = ...\n",
    "count_mean_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Read in the `miles-drives.csv`\n",
    "miles_driven = pd.read_csv('datasets/miles-driven.csv', sep='|')\n",
    "\n",
    "# Merge the `miles_driven` data frame with the `car_acc` data frame\n",
    "car_acc_miles = pd.merge(car_acc, miles_driven, on='state')\n",
    "\n",
    "# Create a new column for the number of drivers involved in fatal accidents\n",
    "car_acc_miles['num_drvr_fatl_col'] = car_acc_miles['drvr_fatl_col_bmiles'] * car_acc_miles['million_miles_annually'] / 1000\n",
    "\n",
    "# Create a barplot of the total number of accidents per cluster\n",
    "sns.barplot(x='cluster', y='num_drvr_fatl_col', data=car_acc_miles, estimator=sum, ci=None)\n",
    "\n",
    "# Calculate the number of states in each cluster and their mean and sum.\n",
    "count_mean_sum = car_acc_miles.groupby('cluster')['num_drvr_fatl_col'].agg(['count', 'mean', 'sum'])\n",
    "count_mean_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose\n",
    "\n",
    "def test_miles_driven():\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(miles_driven, pd.read_csv('datasets/miles-driven.csv', sep='|'))\n",
    "    except AssertionError:\n",
    "        assert False, 'The miles_driven data frame was not read in correctly.'\n",
    "\n",
    "\n",
    "def test_merge_dfs():\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(car_acc_miles.drop(columns='num_drvr_fatl_col'), pd.merge(car_acc, miles_driven, on='state'))\n",
    "    except AssertionError:\n",
    "        assert False, 'The two data frames were not merged correctly'\n",
    "        \n",
    "        \n",
    "def test_new_column():\n",
    "    new_col_df_test = car_acc_miles['drvr_fatl_col_bmiles'] * car_acc_miles['million_miles_annually'] / 1000\n",
    "    new_col_df_test.name = 'num_drvr_fatl_col'\n",
    "    try:\n",
    "        pd.testing.assert_series_equal(car_acc_miles['num_drvr_fatl_col'], new_col_df_test)\n",
    "    except AssertionError:\n",
    "        assert False, 'The new column \"num_drvr_fatl_col\" was not computed correctly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 12. Making a decision when there is no clear right choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "As we can see, there is no obvious correct choice regarding which cluster is the most important to focus on. Yet, we can still argue for a certain cluster and motivate this using our findings above. Which cluster do you think should be a focus for policy intervention and further investigation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "Decide which cluster to focus our resources on.\n",
    "\n",
    "- Which cluster would you choose: 1, 2, or 3?\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "Hints are meant for students who are stuck. Since students can't view solutions in Projects, clicking the hint button is their last resort. We often recommend including code scaffolding (example below).\n",
    "\n",
    "You can read `path_to/my_data.csv` into a DataFrame named `my_data` like this after importing the pandas library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "my_data = pd.read_csv(\"path_to/my_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Which cluster would you choose?\n",
    "cluster_num = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Which cluster would you choose?\n",
    "cluster_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "@tests"
    ]
   },
   "outputs": [],
   "source": [
    "%%nose --nocapture\n",
    "\n",
    "def test_cluster_choice():\n",
    "    assert cluster_num in range(3), \\\n",
    "    'cluster_num must be either 0, 1, or 2'\n",
    "    print('Well done! Note that there is no definite correct answer here and there are a few ways to justify each cluster choice:'\n",
    "          '\\n0 (Blue) = The lowest number of states and the highest number of people helped per state. Good for a focused pilot effort.'\n",
    "          '\\n2 (Green) = The highest number of people helped in total and the most states. Good if we can mobilize many resources right away.'\n",
    "          '\\n1 (Orange) = A good balance of the attributes from the two other clusters. This cluster also has the highest alcohol consumption'\n",
    "          '\\nwhich was the strongest correlated to fatal accidents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The recommended number of tasks in a DataCamp Project is between 8 and 10, so feel free to add more if necessary. You can't have more than 12 tasks.*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
